{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece23403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import hashlib\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import Bucket\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8375e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'Key/pragmatic-bongo-404116-e2d94f71da27.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c2212",
   "metadata": {},
   "source": [
    "#### Create Google Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2b2d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 POST https://storage.googleapis.com/storage/v1/b?project=pragmatic-bongo-404116&prettyPrint=false: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "def create_bucket(bucket_name, project_id):\n",
    "    \"\"\"Creates a new bucket in a specific project.\"\"\"\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    new_bucket = storage_client.create_bucket(bucket, project=project_id)\n",
    "    print(f\"Bucket {new_bucket.name} created\")\n",
    "# Create a new bucket\n",
    "project_id = 'pragmatic-bongo-404116'\n",
    "bucket_name = 'cis4400_group_project'\n",
    "try:\n",
    "    create_bucket(bucket_name, project_id)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96a38a",
   "metadata": {},
   "source": [
    "#### Upload Data to Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f541de82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Data/data.csv uploaded to ownership_payment.csv in bucket cis4400_group_project.\n"
     ]
    }
   ],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name} in bucket {bucket_name}.\")\n",
    "    \n",
    "\n",
    "bucket_name = 'cis4400_group_project'\n",
    "source_file_name = 'Data/data.csv'\n",
    "destination_blob_name = 'ownership_payment.csv'\n",
    "\n",
    "# Upload the CSV file\n",
    "upload_blob(bucket_name, source_file_name, destination_blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca1dc0c",
   "metadata": {},
   "source": [
    "#### Data Transformation (Pulling Data From GCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a229988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your GCS bucket and blob name\n",
    "bucket_name = 'cis4400_group_project'\n",
    "source_blob_name = 'ownership_payment.csv'\n",
    "\n",
    "# Initialize a storage client\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "# Create a function to download the CSV file from GCS into memory\n",
    "def download_blob_to_memory(bucket_name, source_blob_name):\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    data = blob.download_as_bytes()\n",
    "    return io.BytesIO(data)\n",
    "\n",
    "# Download the CSV file from GCS into memory\n",
    "csv_memory = download_blob_to_memory(bucket_name, source_blob_name)\n",
    "\n",
    "# Load the CSV data into a Pandas DataFrame\n",
    "df = pd.read_csv(csv_memory)\n",
    "\n",
    "## Data Transformation\n",
    "\n",
    "# create date_id for date dimension table\n",
    "df['date_id'] = df['payment_publication_date'].str.replace('/', '')\n",
    "# create location_id for location dimension table\n",
    "df['location_id'] = (df['recipient_primary_business_street_address_line1'] + '_' +\n",
    "                    df['recipient_primary_business_street_address_line2'] + '_' +\n",
    "                    df['recipient_city'] + '_' +\n",
    "                    df['recipient_state'] + '_' +\n",
    "                    df['recipient_zip_code'])\n",
    "# make payment publication date a datetime\n",
    "df['payment_publication_date'] = pd.to_datetime(df['payment_publication_date'], format='%m/%d/%Y')\n",
    "# combine primary business street address\n",
    "df['recipient_primary_business_street_address'] = df['recipient_primary_business_street_address_line1'] \\\n",
    "                                                    + df['recipient_primary_business_street_address_line2']\n",
    "# make dispute_status_for_publication a boolean column instead of yes's and no's\n",
    "df['dispute_status_for_publication'] = df['dispute_status_for_publication'].map({'Yes': True, 'No': False})\n",
    "# change column name for physician first, middle, and last\n",
    "df.rename(columns={'physician_first_name': 'physician_Firstname', 'physician_middle_name': 'physician_Middlename', \n",
    "                   'physician_last_name': 'physician_Lastname'}, inplace=True)\n",
    "# make all column names caps\n",
    "df.columns = [col.upper() for col in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10bd696",
   "metadata": {},
   "source": [
    "#### Create BigQuery Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2716ddf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 POST https://bigquery.googleapis.com/bigquery/v2/projects/pragmatic-bongo-404116/datasets?prettyPrint=false: Already Exists: Dataset pragmatic-bongo-404116:ownership_payment\n"
     ]
    }
   ],
   "source": [
    "def create_bigquery_dataset(project_id, dataset_name):\n",
    "    \"\"\"Creates a BigQuery dataset.\"\"\"\n",
    "    bigquery_client = bigquery.Client(project=project_id)\n",
    "    dataset_id = f\"{project_id}.{dataset_name}\"\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = \"US\"\n",
    "    bigquery_client.create_dataset(dataset)\n",
    "    print(f\"Dataset {dataset_id} created.\")\n",
    "\n",
    "project_id = 'pragmatic-bongo-404116'\n",
    "dataset_name = 'ownership_payment'\n",
    "try:\n",
    "    create_bigquery_dataset(project_id, dataset_name)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b26e65",
   "metadata": {},
   "source": [
    "#### Create Tables in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b3f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Payment_Fact already exists in the dataset ownership_payment.\n",
      "Table DimLocation already exists in the dataset ownership_payment.\n",
      "Table DimPhysician already exists in the dataset ownership_payment.\n",
      "Table DimManufacturerOrGpaPayer already exists in the dataset ownership_payment.\n",
      "Table DimDate already exists in the dataset ownership_payment.\n"
     ]
    }
   ],
   "source": [
    "# Get the path to the service account key file from the environment variable\n",
    "service_account_path = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "\n",
    "# Set your Google Cloud credentials using the environment variable\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Define your dataset and table names\n",
    "dataset_name = 'ownership_payment'\n",
    "fact_table_name = 'Payment_Fact'\n",
    "location_dim_table_name = 'DimLocation'\n",
    "physician_dim_table_name = 'DimPhysician'\n",
    "manufacturer_dim_table_name = 'DimManufacturerOrGpaPayer'\n",
    "date_dim_table_name = 'DimDate'\n",
    "\n",
    "# Create the dataset\n",
    "dataset_ref = client.dataset(dataset_name)\n",
    "client.get_dataset(dataset_ref)\n",
    "\n",
    "# Define the schema for the fact table\n",
    "fact_table_schema = [\n",
    "    bigquery.SchemaField('RECORD_ID', 'INTEGER', mode='REQUIRED'),\n",
    "    bigquery.SchemaField('LOCATION_ID', 'STRING'),\n",
    "    bigquery.SchemaField('PHYSICIAN_PROFILE_ID', 'INTEGER'),\n",
    "    bigquery.SchemaField('APPLICABLE_MANUFACTURER_OR_APPLICABLE_GPO_MAKING_PAYMENT_ID', 'INTEGER'),\n",
    "    bigquery.SchemaField('DATE_ID', 'STRING'),\n",
    "    bigquery.SchemaField('VALUE_OF_INTEREST', 'FLOAT'),\n",
    "    bigquery.SchemaField('TOTAL_AMOUNT_INVESTED_USDOLLARS', 'FLOAT'),\n",
    "    bigquery.SchemaField('DISPUTE_STATUS_FOR_PUBLICATION', 'BOOL') \n",
    "]\n",
    "\n",
    "# DEFINE SCHEMA FOR REST OF DIMENSION TABLES AND FACT TABLE BASED ON DATA MODEL\n",
    "location_dim_table_schema = [\n",
    "    bigquery.SchemaField('LOCATION_ID', 'STRING', mode='REQUIRED'),\n",
    "    bigquery.SchemaField('RECIPIENT_PRIMARY_BUSINESS_STREET_ADDRESS', 'STRING'),\n",
    "    bigquery.SchemaField('RECIPIENT_CITY', 'STRING'),\n",
    "    bigquery.SchemaField('RECIPIENT_STATE', 'STRING'),\n",
    "    bigquery.SchemaField('RECIPIENT_ZIP_CODE', 'STRING'),\n",
    "    bigquery.SchemaField('RECIPIENT_COUNTRY', 'STRING'),\n",
    "    bigquery.SchemaField('RECIPIENT_PROVINCE', 'STRING'),\n",
    "    bigquery.SchemaField('RECIPIENT_POSTAL_CODE', 'STRING')\n",
    "]\n",
    "physician_dim_table_schema = [\n",
    "    bigquery.SchemaField('PHYSICIAN_PROFILE_ID', 'STRING', mode='REQUIRED'),\n",
    "    bigquery.SchemaField('PHYSICIAN_FIRSTNAME', 'STRING'),\n",
    "    bigquery.SchemaField('PHYSICIAN_MIDDLENAME', 'STRING'),\n",
    "    bigquery.SchemaField('PHYSICIAN_LASTNAME', 'STRING'),\n",
    "    bigquery.SchemaField('PHYSICIAN_NPI', 'STRING'),\n",
    "    bigquery.SchemaField('PHYSICIAN_PRIMARY_TYPE', 'STRING'),\n",
    "    bigquery.SchemaField('PHYSICIAN_SPECIALTY', 'STRING'),\n",
    "    bigquery.SchemaField('PHYSICIAN_NAME_SUFFIX', 'STRING'),\n",
    "    bigquery.SchemaField('INTEREST_HELD_BY_PHYSICIAN_OR_AN_IMMEDIATE_FAMILY_MEMBER', 'STRING')\n",
    "\n",
    "]\n",
    "manufacturer_dim_table_schema = [\n",
    "    bigquery.SchemaField('APPLICABLE_MANUFACTURER_OR_APPLICABLE_GPO_MAKING_PAYMENT_ID', 'STRING', mode='REQUIRED'),\n",
    "    bigquery.SchemaField('SUBMITTING_APPLICABLE_MANUFACTURER_OR_APPLICABLE_GPO_NAME', 'STRING'),\n",
    "    bigquery.SchemaField('APPLICABLE_MANUFACTURER_OR_APPLICABLE_GPO_MAKING_PAYMENT_NAME', 'STRING'),\n",
    "    bigquery.SchemaField('APPLICABLE_MANUFACTURER_OR_APPLICABLE_GPO_MAKING_PAYMENT_STATE', 'STRING'),\n",
    "    bigquery.SchemaField('APPLICABLE_MANUFACTURER_OR_APPLICABLE_GPO_MAKING_PAYMENT_COUNTRY', 'STRING')\n",
    "\n",
    "]\n",
    "date_dim_table_schema = [\n",
    "    bigquery.SchemaField('DATE_ID', 'STRING', mode = 'REQUIRED'),\n",
    "    bigquery.SchemaField('PAYMENT_PUBLICATION_DATE', 'DATE'),\n",
    "    bigquery.SchemaField('PROGRAM_YEAR', 'INTEGER')\n",
    "]\n",
    "\n",
    "\n",
    "# Create the tables\n",
    "fact_table_ref = dataset_ref.table(fact_table_name)\n",
    "try:\n",
    "    client.get_table(fact_table_ref)\n",
    "    print(f\"Table {fact_table_name} already exists in the dataset {dataset_name}.\")\n",
    "except:\n",
    "    fact_table = bigquery.Table(fact_table_ref, schema=fact_table_schema)\n",
    "    client.create_table(fact_table)\n",
    "    print(f\"{fact_table_name} Created\")\n",
    "    \n",
    "location_dim_table_ref = dataset_ref.table(location_dim_table_name)\n",
    "try:\n",
    "    client.get_table(location_dim_table_ref)\n",
    "    print(f\"Table {location_dim_table_name} already exists in the dataset {dataset_name}.\")\n",
    "except:\n",
    "    location_dim_table = bigquery.Table(location_dim_table_ref, schema=location_dim_table_schema)\n",
    "    client.create_table(location_dim_table)\n",
    "    print(f\"{location_dim_table_name} Created\")\n",
    "    \n",
    "physician_dim_table_ref = dataset_ref.table(physician_dim_table_name)\n",
    "try:\n",
    "    client.get_table(physician_dim_table_ref)\n",
    "    print(f\"Table {physician_dim_table_name} already exists in the dataset {dataset_name}.\")\n",
    "except:\n",
    "    physician_dim_table = bigquery.Table(physician_dim_table_ref, schema=physician_dim_table_schema)\n",
    "    client.create_table(physician_dim_table)\n",
    "    print(f\"{physician_dim_table_name} Created\")\n",
    "    \n",
    "manufacturer_dim_table_ref = dataset_ref.table(manufacturer_dim_table_name)\n",
    "try:\n",
    "    client.get_table(manufacturer_dim_table_ref)\n",
    "    print(f\"Table {manufacturer_dim_table_name} already exists in the dataset {dataset_name}.\")\n",
    "except:\n",
    "    manufacturer_dim_table = bigquery.Table(manufacturer_dim_table_ref, schema=manufacturer_dim_table_schema)\n",
    "    client.create_table(manufacturer_dim_table)\n",
    "    print(f\"{manufacturer_dim_table_name} Created\")\n",
    "    \n",
    "date_dim_table_ref = dataset_ref.table(date_dim_table_name)\n",
    "try:\n",
    "    client.get_table(date_dim_table_ref)\n",
    "    print(f\"Table {date_dim_table_name} already exists in the dataset {dataset_name}.\")\n",
    "except:\n",
    "    date_dim_table = bigquery.Table(date_dim_table_ref, schema=date_dim_table_schema)\n",
    "    client.create_table(date_dim_table)\n",
    "    print(f\"{date_dim_table_name} Created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f74f2",
   "metadata": {},
   "source": [
    "#### Load Data To BigQuery Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c19bc926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded Data To pragmatic-bongo-404116.ownership_payment.Payment_Fact\n",
      "Uploaded Data To pragmatic-bongo-404116.ownership_payment.DimLocation\n",
      "Uploaded Data To pragmatic-bongo-404116.ownership_payment.DimPhysician\n",
      "Uploaded Data To pragmatic-bongo-404116.ownership_payment.DimManufacturerOrGpaPayer\n",
      "Uploaded Data To pragmatic-bongo-404116.ownership_payment.DimDate\n"
     ]
    }
   ],
   "source": [
    "# Function to upload data to BigQuery from a DataFrame\n",
    "def upload_data_from_dataframe(df, table_ref):\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    job_config.autodetect = True\n",
    "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "    job.result()  # Wait for the job to complete\n",
    "    print(f\"Uploaded Data To {table_ref}\")\n",
    "    \n",
    "# Split your DataFrame into the respective dimension and fact DataFrames\n",
    "def split_df(df):\n",
    "    fact_cols = ['RECORD_ID', \n",
    "                 'LOCATION_ID', \n",
    "                 'PHYSICIAN_PROFILE_ID', \n",
    "                 'APPLICABLE_MANUFACTURER_OR_APPLICABLE_GPO_MAKING_PAYMENT_ID', \n",
    "                 'DATE_ID',\n",
    "                 'VALUE_OF_INTEREST', \n",
    "                 'TOTAL_AMOUNT_INVESTED_USDOLLARS', \n",
    "                 'DISPUTE_STATUS_FOR_PUBLICATION'\n",
    "                ]\n",
    "    \n",
    "    location_dim_cols = [\n",
    "                    \"LOCATION_ID\",\n",
    "                    \"RECIPIENT_PRIMARY_BUSINESS_STREET_ADDRESS\",\n",
    "                    \"RECIPIENT_CITY\",\n",
    "                    \"RECIPIENT_STATE\",\n",
    "                    \"RECIPIENT_ZIP_CODE\",\n",
    "                    \"RECIPIENT_COUNTRY\",\n",
    "                    \"RECIPIENT_PROVINCE\",\n",
    "                    \"RECIPIENT_POSTAL_CODE\"\n",
    "                ]\n",
    "    \n",
    "    physician_dim_cols = [\n",
    "                    \"PHYSICIAN_PROFILE_ID\",\n",
    "                    \"PHYSICIAN_FIRSTNAME\",\n",
    "                    \"PHYSICIAN_MIDDLENAME\",\n",
    "                    \"PHYSICIAN_LASTNAME\",\n",
    "                    \"PHYSICIAN_NPI\",\n",
    "                    \"PHYSICIAN_PRIMARY_TYPE\",\n",
    "                    \"PHYSICIAN_SPECIALTY\",\n",
    "                    \"PHYSICIAN_NAME_SUFFIX\",\n",
    "                    \"INTEREST_HELD_BY_PHYSICIAN_OR_AN_IMMEDIATE_FAMILY_MEMBER\"\n",
    "                ]\n",
    "\n",
    "    \n",
    "    manufacturer_dim_cols = [\n",
    "                    \"APPLICABLE_MANUFACTURER_OR_APPLICABLE_GPO_MAKING_PAYMENT_ID\",\n",
    "                    \"SUBMITTING_APPLICABLE_MANUFACTURER_OR_APPLICABLE_GPO_NAME\",\n",
    "                    \"APPLICABLE_MANUFACTURER_OR_APPLICABLE_GPO_MAKING_PAYMENT_NAME\",\n",
    "                    \"APPLICABLE_MANUFACTURER_OR_APPLICABLE_GPO_MAKING_PAYMENT_STATE\",\n",
    "                    \"APPLICABLE_MANUFACTURER_OR_APPLICABLE_GPO_MAKING_PAYMENT_COUNTRY\"\n",
    "                ]\n",
    "\n",
    "    \n",
    "    date_dim_cols = [\"DATE_ID\", \"PAYMENT_PUBLICATION_DATE\", \"PROGRAM_YEAR\"]\n",
    "    \n",
    "    fact_df = df[fact_cols]\n",
    "    location_dim_df = df[location_dim_cols]\n",
    "    physician_dim_df = df[physician_dim_cols]\n",
    "    manufacturer_dim_df = df[manufacturer_dim_cols]\n",
    "    date_dim_df = df[date_dim_cols]\n",
    "    \n",
    "    return fact_df, location_dim_df, physician_dim_df, manufacturer_dim_df, date_dim_df\n",
    "\n",
    "fact_df, location_dim_df, physician_dim_df, manufacturer_dim_df, date_dim_df = split_df(df)\n",
    "\n",
    "# Upload the data to BigQuery\n",
    "upload_data_from_dataframe(fact_df, fact_table_ref)\n",
    "upload_data_from_dataframe(location_dim_df, location_dim_table_ref)\n",
    "upload_data_from_dataframe(physician_dim_df, physician_dim_table_ref)\n",
    "upload_data_from_dataframe(manufacturer_dim_df, manufacturer_dim_table_ref)\n",
    "upload_data_from_dataframe(date_dim_df, date_dim_table_ref)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
